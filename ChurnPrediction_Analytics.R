#Getting the current working directory
getwd()


#Changing the directory location to file location
setwd("C:/Users/pranavi/Downloads")


#Loading the dataset
df=read.csv("Telco-Customer-Churn.csv")


#Installing Tidyverse and miscset package.
install.packages("tidyverse")
library(tidyverse)
install.packages("miscset")
library(miscset)

#Dimentions of the data
dim(df)
fix(df)

#Names of the columns
names(df)


#Get the data types of the features

glimpse(df)

#For now, let's start by transforming the character variables, 
#as well as the "SeniorCitizen"" variable, to factor types.



df=df %>% mutate_if(is.character,is.factor)
df$SeniorCitizen <- as.factor(df$SeniorCitizen)
glimpse(df$SeniorCitizen)


#Now lets look for missing values.

df %>% map(~ sum(is.na(.)))

#It looks like "TotalCharges" is the only feature with missing values. 
#Lets go ahead and impute the 11 missing values using the median value.

# imputing with the median
df <- df %>%
  mutate(TotalCharges = replace(TotalCharges,
                                is.na(TotalCharges),
                                median(TotalCharges, na.rm = T)))
# checking that the imputation worked
sum(is.na(df$TotalCharges))
glimpse(df)

#Exploring the data

df_tbl <- df %>%
  select_if(is.factor) %>%
  summarise_all(n_distinct) 
#Let's start by taking a look at the unique values of the factor variables.
df_tbl
df_tbl[1:18] %>%
     print(width = Inf)
#here's a unique value for each "customerID" 
#so we probably won't be able to gain much information there. 
#All of the other factors have four or fewer unique values, 
#so they will all be pretty manageable.




#To guide the analysis, I'm going to try and answer the following questions about my customer segments:
  #Are men more likely to churn than women?
  #Are senior citizens more like to churn?
  #Do individuals with a partner churn more than those without a partner?
  #Do people with dependents churn more than people that do not have dependents?



#I'll start with gender. I wouldn't expect one gender to be more likely than another to churn, but lets see what the data shows.


ggplot(df) +
  geom_bar(aes(x = gender, fill = Churn), position = "dodge")
#Taking a look, the results are similar. Roughly one quarter of the male customers churn, and roughly one quarter of the female customers churn.
#We can also take a look at exactly how many people from each gender churned.



df %>%
  group_by(gender,Churn) %>%
  summarise(n=n())

#Next I'll take a look at senior citizens.
ggplot(df)+geom_bar(aes(x=SeniorCitizen, fill= Churn),position = "dodge")

df %>%group_by(SeniorCitizen) %>%summarise(n = n()) %>%mutate(freq = n / sum(n))
df %>%group_by(SeniorCitizen, Churn) %>%summarise(n = n()) %>%mutate(freq = n / sum(n))


#This variable shows a much more meaningful relationship. 
#Roughly 16% of the customers are senior citizens, and roughly 42% of those senior citizens churn.
#On the other hand, of the 84% of customers that are not senior citizens, only 24% churn. 
#These results show that senior citizens are much more likely to churn.


#Now I'm going to take a look at people with partners.

ggplot(df) +
  geom_bar(aes(x=Partner, fill = Churn), position = "dodge")
df%>% group_by(Partner)%>%summarise(n = n()) %>% mutate(freq = n / sum(n))
df %>% group_by(Partner, Churn) %>% summarise(n = n()) %>% mutate(freq = n / sum(n))

#Roughly half of the people have partners. Of the people with partners, 
#20% churn. For people without partners, approximately 33% churn.


#Next, I'll take a look at the Dependents category.

ggplot(df) +
  geom_bar(aes(x=Dependents, fill = Churn), position = "dodge")
df %>% group_by(Dependents, Churn) %>% summarise(n=n()) %>% mutate(freq = n / sum(n))
df %>% group_by(Dependents) %>% summarise(n = n()) %>% mutate(freq = n / sum(n))
#Approximately 30% of the people have dependents, of which 15% churn. 
#For the other 70% that don't have dependents, 31% churn.


#Another useful visualization is the box and whisker plot. 
#This gives us a little bit more compact visual of our data,and helps us identify outliers. 
#Lets take a look at some box and whisker plots for total charges of the different customer segments.


ggplot(df, aes(x = SeniorCitizen, y = TotalCharges)) +geom_boxplot()
ggplot(df, aes(x = Dependents, y = TotalCharges)) +geom_boxplot()
ggplot(df, aes(x = Partner, y = TotalCharges)) +geom_boxplot()

#After looking at these initial results, we can ask some more questions.
#We might want to compare the total charges of senior citizens, people without partners, 
#and people without dependents.


#These seem to be the subsets of people most likely to churn within their respective customer segments.
#Lets compare them so that we can identify where we would potentially focus our efforts.

df %>%
  select(SeniorCitizen, Churn, TotalCharges, tenure) %>%
  filter(SeniorCitizen == 1, Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)

df %>%
  select(Partner, Churn, TotalCharges, tenure) %>%
  filter(Partner == "No", Churn == "Yes") %>%
  summarise(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
df %>%
  select(Dependents, Churn, TotalCharges, tenure) %>%
  filter(Dependents == "No", Churn == "Yes") %>%
  summarise(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)


#Based on the results, we should focus our efforts on people without dependents. 
#This customer segment that churned had nearly 2.3MM in total charges compared to 1.3MM 
#for people without partners, and only 900K for senior citizens.

#Let's dig a little deeper and see what services that customer segment uses.


dependents <- df %>% filter(Dependents == "No")

ggplotGrid(ncol=2,
           lapply(c("PhoneService","MultipleLines","InternetService","OnlineSecurity","OnlineBackup",
                    "DeviceProtection"),
                  function(col){
                    ggplot(dependents,aes_string(col)) + geom_bar(aes(fill=Churn),position="dodge")
                  }))

ggplot(dependents) +
  geom_bar(aes(x=PaymentMethod,fill=Churn), position = "dodge")

#Taking a look at the results, we gain some potential insights. 
#Based on these insights, here are some recommendations for improving 
#customer retention:
#1.A lot of people with phone service churned. Maybe these people don't really use the phone service.
#Moving them to a plan without phone service to save them some money on their bill might help retain them.



#Predictive Models:
#__________________________________________
install.packages('caret')
library(caret)
# removing customerID; doesn't add any value to the model
df <- df %>% select(-customerID)  
# train/test split; 75%/25%
# setting the seed for reproducibility
set.seed(5)
inTrain <- createDataPartition(y = df$Churn, p=0.75, list=FALSE)
train <- df[inTrain,]
test <- df[-inTrain,]
View(train)
#Now that the data is split, I'll fit a logistic regression model using all of
#the features. After I fit the model, I'll take a look at the
#confusion matrix to see how well the model made predictions on the 
#validation set.


# fitting the model
fit <- glm(Churn~., data=train, family=binomial)
#making prediction
churn.probs <- predict(fit, test, type="response")
head(churn.probs)


# converting probabilities to classes; "Yes" or "No"
contrasts(df$Churn)  # Yes = 1, No = 0
glm.pred = rep("No", length(churn.probs))
glm.pred[churn.probs > 0.5] = "Yes"
test$Churn
glm.pred
confusionMatrix(glm.pred, test$Churn, positive = "Yes")
glimpse(glm.pred)
contrasts(test$Churn)
glimpse(test$Churn)
pred=as.factor(glm.pred)
conf_matrix = confusionMatrix(pred, test$Churn, positive = "Yes")
conf_matrix
#Right out of the box, it looks like the model is performing fairly well. 
#The accuracy is 81%. If we were to predict that all results in the test set were 
#the majority class (No), the accuracy would be 73%


#Some of the other metrics that are reported are better measures though,
#because the response classes are slightly imbalanced (~73% = No, ~27% = Yes).
#The sensitivity,which is a measure of the true positive rate (TP/(TP+FN)),is 56%.
#The specificity, or true negative rate (TN/(TN+FP)), is 90%.


#This tells us that our model is 56% accurate at correctly identifying true positives.
#Phrased another way,the model has correctly identified 56% of people 
#that actually churned.

#Another useful metric is AUC. 
#This is the area under the receiver operating characteristic (ROC) curve.
#By default, I used 0.5 as the threshold for making predictions from the probabilities. 
#Often times this isn't optimal, so the ROC curve is constructed to plot true positive rate vs. the false positive rate (y=TP, x=FP).




#AUC can take on any value between 0 and 1. 
#The baseline model used is a random predictor, which has a value of 0.5. 
#The further this value is from 0.5, the better, with an ideal model having 
#an AUC of 1.


#Now I'll take a look at the ROC curve and the AUC value.

install.packages('ROCR')
library(ROCR)
# need to create prediction object from ROCR
pr <- prediction(churn.probs, test$Churn)
churn.probs

# plotting ROC curve
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)


# AUC value
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
#The baseline performance from random guessing provides a 45 degree line, 
#so we can see that our model is outperforming random guessing,
#which is good. 


#Feature Selection
#If we want to try and improve our model, we can take a look at the summary 
#of the fit and identify which features are significant (p-value < 0.05).
#We can also use the varImp function, 
#which takes the absolute value of the test statistic(higher number,the better).
#The AUC measure is 0.85, which is greater than 0.5 (baseline model)
#which is also good.

# summary of the model
summary(fit)
# feature importance
feature_importance=as.vector(unlist(varImp(fit)))
sort(varImp(fit),decreasing = TRUE)

sort(feature_importance, decreasing = TRUE)
str(feature_importance)



#I'm using a p-value of 0.05 as my threshold (95% confidence interval) 
#for the coefficient estimates, which is 1.96 standard deviations from 
#the mean, so this will be my cutoff for which features to include. 
#Now lets fit a model with those features and see how it compares.


# fitting the model
fit <- glm(Churn~SeniorCitizen + tenure + MultipleLines + InternetService + StreamingTV + Contract + PaperlessBilling + PaymentMethod + TotalCharges
           , data=train,
           family=binomial)
# making predictions
churn.probs <- predict(fit, test, type="response")
head(churn.probs)
# converting probabilities to classes; "Yes" or "No"
contrasts(df$Churn)  # Yes = 1, No = 0
glm.pred = rep("No", length(churn.probs))
glm.pred[churn.probs > 0.5] = "Yes"
glm.pred=as.factor(glm.pred)
confusionMatrix(glm.pred, test$Churn, positive = "Yes")
#The accuracy has remained virtually unchanged, with a value of 80%. 
#Similarly, the true positive rate (55%) and true negative rate (89%) 
#haven't changed.

#The true advantage of simplifying down the model and excluding those features
#is interpretability. With multicollinearity, the coefficient estimates are 
#unstable, so depending on our sample, they can change drastically. 
#Simplifying the model down and attempting to exclude some of this 
#multicollinearity makes the estimates more stable.

#We can see evidence of this in the standard error. 
#The InternetService feature has a standard error of 0.9 in the original model,
#but in the simplified model its reduced to 0.1. 
#This tells us that our second, more simplified, 
#model has much more stable coefficient estimates.


#Now lets also take a look at the ROC curve, and AUC.

library(ROCR)
# need to create prediction object from ROCR
pr <- prediction(churn.probs, test$Churn)

# plotting ROC curve
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)


# AUC value
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc




#Random Forest
#I'm going to be using a random forest model, because it can deal with 
#non-linearities better than logistic regression. 
#I'm not really sure if my data has strong non-linear relationships,
#but if the random forest model outperforms logistic regression, it might.


#Random forest uses multiple decision trees to make predictions. 
#Single decision trees on their own can be very effective at learning 
#non-linear relationships (low bias, but high variance). 
#Due to their high variance, they can tend to over-fit. 
#Random forest reduces this variance by averaging many trees 
#(at the sacrifice of a slight increase in the bias).


#I'll start by fitting the model to all of the features.
install.packages('randomForest')
library(randomForest)
churn.rf = randomForest(Churn~., data = train, importance = T)

churn.rf


churn.predict.prob <- predict(churn.rf, test, type="prob")

churn.predict <- predict(churn.rf, test)
confusionMatrix(churn.predict, test$Churn, positive = "Yes")

#The accuracy of the model is 79%, the true positive rate is 52%, 
#and the true negative rate is 89%. It looks like the model performed
#slightly worse than logistic regression, but not by much.
#Now I'll take a look at the ROC curve and AUC.
library(ROCR)
# need to create prediction object from ROCR
pr <- prediction(churn.predict.prob[,2], test$Churn)

# plotting ROC curve
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)



auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
#The AUC is 0.84, which is pretty close to the logistic regression model. 
#Now I'll take a look at the feature importance of the variables in the random forest model.

importance(churn.rf)

varImpPlot(churn.rf)
  

#There's two measures of feature importance that are reported, 
#mean decrease in accuracy, and mean decrease in gini.

#The first is the decrease in accuracy of out of bag samples 
#when the variable feature is excluded from the model.

#The second is the mean decrease in gini. 
#This metric has to do with the decrease in node impurity that results from splits over that variable.
#The higher the mean decrease in gini, the lower the node impurity. 
#Basically, this means that the lower the node impurity, 
#the more likely the split will produce a left node that is dedicated to one class, 
#and a right node that is dedicated to another class. 
#If the split is totally pure, the left node will be 100% of one class, 
#and the right will be 100% of another class. 
#This is obviously more optimal for making predictions than having two nodes of
#mixed classes.


#Some of the features that were important in the logistic regression model, 
#such as tenure and TotalCharges,are also important to the random forest model.
#other features like TechSupport and MonthlyCharges were not significant in the
#logistic regression model, but are ranked fairly high for the random forest model.
   

#Parameter Tuning
#________________
#Rather than try a different subset of features,
#I'll try to tune some of the parameters of the random forest model. 
#First I'll change the number of variables that are sampled at each split. 
#Right now the default is 4, so I'll try several other numbers, 
#and use the AUC as the comparison metric


# changing the number of variables to try at each split
# mtry = 8, 12, 16, 20.

# fitting the model
churn.rf = randomForest(Churn~., data = train, mtry = 20, importance = T)

churn.predict.prob <- predict(churn.rf, test, type="prob")

churn.predict <- predict(churn.rf, test)
confusionMatrix(churn.predict, test$Churn, positive = "Yes")


# need to create prediction object from ROCR
pr <- prediction(churn.predict.prob[,2], test$Churn)

# AUC value
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
#Changing the number of variables to try didn't improve the model.
#The resulting AUC decreased slightly to 0.83.
#Now I'll try changing the number of trees.

# changing the number of trees
# ntree = 25, 250, 500, 750

# fitting the model
churn.rf = randomForest(Churn~., data = train, ntree = 750, importance = T)

churn.predict.prob <- predict(churn.rf, test, type="prob")

churn.predict <- predict(churn.rf, test)
confusionMatrix(churn.predict, test$Churn, positive = "Yes")


# need to create prediction object from ROCR
pr <- prediction(churn.predict.prob[,2], test$Churn)

# AUC value
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


# ntree = 25; AUC = 0.83
# ntree = 250; AUC = 0.84
# ntree = 500; AUC = 0.84
# ntree = 750; AUC = 0.84


#Changing the number of trees didn't really improve the model either,
#so I'll stick with the original model (mtry = 4, ntree = 500).
#Finally, I'll use K-fold cross-validation with 10 folds, repeated 3 times,
#to compare the models.

#K-fold Cross Validation
#________________________

#In the previous sections I used a train/test validation procedure for evaluating my models. 
#In this section, I'll vet the models a little more rigorously using 10-fold 
#cross validation, repeated 3 times.



#k-fold cross val in caret
set.seed(10)
# train control
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated 3 times
  repeats = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary)
# logistic regression model
logreg <- train(Churn ~., df,
                method = "glm",
                family = "binomial",
                trControl = fitControl,
                metric = "ROC")
## ROC        Sens       Spec     
## 0.8453004  0.8960842  0.5519886
# random forest model
rf <- train(Churn ~., df,
            method = "rf",
            trControl = fitControl,
            metric = "ROC")

#The results are very similar to the previous results.
#The logistic regression model had an AUC of approximately 0.84, 
#and the random forest model had an AUC of approximately 0.83.


##Cost Evaluation
#________________


#All of the previous modeling and evaluation metrics were useful,
#but they don't tell us much about the actual impacts on the business. 
#In this section, I'll go over the cost implications of implementing, vs.
#not implementing a predictive model.

#To start, I'll make several assumptions related to cost. 
#Doing a quick search, it looks like the customer acquisition cost 
#in the telecom industry is around Rs300. 
#I'll assume that this is the customer acquisition cost in my model as a 
#result of false negative predictions (predicting that a customer was happy, 
#but the customer actually churned).

#Doing another quick search, it looks like customer acquisition cost is 
#approximately five times higher than customer retention costs. 
#I'll assume that my customer retention costs are Rs60. 
#These costs will be incurred during false positives(predicting a 
#customer would churn when they were actually happy), 
#and true positives (predicting unhappy customers correctly). 
#There will be no cost incurred for true negative predictions
#(correctly predicting a customer was happy).

#Here's the equation for cost that I'm going to try and minimize:
#cost = FN(300) + TP(60) + FP(60) + TN(0)
#Since the logistic regression model seemed to perform slightly better,
#I'll use that model.


# fitting the logistic regression model
fit <- glm(Churn~., data=train, family=binomial)
# making predictions
churn.probs <- predict(fit, test, type="response")
head(churn.probs)
# converting probabilities to classes; "Yes" or "No"
contrasts(df$Churn)  # Yes = 1, No = 0
glm.pred = rep("No", length(churn.probs))
glm.pred[churn.probs > 0.5] = "Yes"
glimpse(pred)
pred=as.factor(glm.pred)
glimpse(test$Churn)

x <- confusionMatrix(pred, test$Churn, positive = "Yes")
# cost as a function of threshold
thresh <- seq(0.1,1.0, length = 10)
cost = rep(0,length(thresh))
for (i in 1:length(thresh))
{
  pred = rep("No", length(churn.probs))
  pred[churn.probs > thresh[i]] = "Yes"
  x <- confusionMatrix(as.factor(pred), test$Churn, positive = "Yes")
  TN <- x$table[1]/1760
  FP <- x$table[2]/1760
  FN <- x$table[3]/1760
  TP <- x$table[4]/1760
  cost[i] = FN*300 + TP*60 + FP*60 + TN*0
}
# simple model - assume threshold is 0.5
glm.pred = rep("No", length(churn.probs))
glm.pred[churn.probs > 0.5] = "Yes"
x <- confusionMatrix(as.factor(glm.pred), test$Churn, positive = "Yes")
TN <- x$table[1]/1760
FP <- x$table[2]/1760
FN <- x$table[3]/1760
TP <- x$table[4]/1760
cost_simple = FN*300 + TP*60 + FP*60 + TN*0



# putting results in a dataframe for plotting
dat <- data.frame(
  model = c(rep("optimized",10),"simple"),
  cost_thresh = c(cost,cost_simple),
  thresh_plot = c(thresh,0.5)
)


ggplot(dat, aes(x = thresh_plot, y = cost_thresh, group = model, colour = model)) +
  geom_line() +geom_point()


# cost savings of optimized model (threshold = 0.2) compared to baseline model (threshold = 0.5)

savings_per_customer = cost_simple - min(cost)

total_savings = 500000*savings_per_customer


## total savings:  4107955
#If we assume that our baseline model is the logistic regression model 
#with a threshold of 0.5,the cost associated with this model is Rs48/customer.
#If we optimize the model and use a threshold of 0.2,
#our customer retention cost is reduced to Rs40/customer
#Assuming a customer base of 500,000 this comes out to a yearly savings 
#of over Rs.40lacks.

#This example illustrates the value of optimizing a machine learning model 
#for accuracy,as well as impact on the business.
